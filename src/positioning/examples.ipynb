{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A showcase of the different functionalities of the framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 1: Gather the Test Inputs**\n",
    "  - The test session with all the pictures and coordinates\n",
    "  - The global coordinates of the device\n",
    "- **Step 2: check for relevant Reference data**\n",
    "  - use the global coordinates to find all the reference data that is geo-referenced close enough (GPS precision)\n",
    "- **Step 3: 2D Check**\n",
    "  - Compare all the test images against all the reference images\n",
    "  - Find which session has the highest match rate\n",
    "  - Find which Image has the highest match rate\n",
    "  - Calculate the transformation between the two images\n",
    "  - calculate the inverse transformation to give the test data a Reference global position\n",
    "- **Step 4: 3D Check**\n",
    "  - Compare the test mesh against relevant point clouds\n",
    "  - Compare the test mesh against BIM models\n",
    "  - Perform a CCP for the final alignment\n",
    "- **Step 5: Choosing final Position**\n",
    "  - Use the different results from all the methods to come to a best position\n",
    "  - Send the Position and rotation back to the device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Functionality\n",
    "A session contains all the data from the folder: the images, meshes and their locations. It also contains the geo-reference position and rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Selecting Sessions\n",
    "Sessions can either be directly imported from a path, or be selected from a parent directory based on the distance to the reference point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import session\n",
    "\n",
    "# you can import all the close enough sessions from a parent directory \n",
    "sessionsFolderLocation = \"/Volumes/GeomaticsProjects1/Projects/2025-03 Project FWO SB Jelle/7.Data/21-11 Testbuilding Campus/RAW Data\"\n",
    "referencePoint = np.array([0,0,0])\n",
    "maxDistance = 10\n",
    "sessions = session.find_close_sessions(sessionsFolderLocation, referencePoint, maxDistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also import a single session from the directory\n",
    "sessionDirectory = \"/Volumes/GeomaticsProjects1/Projects/2025-03 Project FWO SB Jelle/7.Data/21-11 Testbuilding Campus/RAW Data/Hololens/session-2021-11-25 16-09-47\"\n",
    "singleSession = session.Session().from_path(sessionDirectory)\n",
    "print (singleSession.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Area\n",
    "Each session has a bounding area to determine if the reference position is close enough to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundingBox = singleSession.get_bounding_box()\n",
    "boundingRadius = singleSession.get_bounding_radius()\n",
    "\n",
    "print(\"The axis aligned bounding box (2x3 array) min and max corner points \", boundingBox)\n",
    "print (\"The bounding radius from the reference Center:\", boundingRadius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image transforms\n",
    "The image transform contains the Image file, and it's transform in session space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an image transform is stored in \n",
    "image1 = singleSession.imageTransforms[0]\n",
    "print(image1.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Image\n",
    "The image is stored as an openCV color image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "cv2Image = image1.get_cv2_image()\n",
    "\n",
    "plt.imshow(cv2.cvtColor(cv2Image, cv2.COLOR_BGR2RGB))\n",
    "plt.title('cv2 Color Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning3D as pos3D\n",
    "import session\n",
    "\n",
    "sessionDirectory = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/session-2021-11-25 16-17-19\"\n",
    "singleSession = session.Session().from_path(sessionDirectory)\n",
    "#print(singleSession.geometries)\n",
    "pos3D.show_geometries(singleSession.geometries, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positioning 2D Functionality\n",
    "All the different methods and functions to calculate the relative position of a Session using 2D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Images\n",
    "2 images can be compared against each other to find the corresponding matches and quality of the match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import 2 images from a session\n",
    "sessionDirectory = \"/Volumes/GeomaticsProjects1/Projects/2025-03 Project FWO SB Jelle/7.Data/21-11 Testbuilding Campus/RAW Data/Hololens/session-2021-11-25 16-09-47\"\n",
    "singleSession = session.Session().from_path(sessionDirectory)\n",
    "image1 = singleSession.imageTransforms[0]\n",
    "image2 = singleSession.imageTransforms[1]\n",
    "image3 = singleSession.imageTransforms[2]\n",
    "\n",
    "\n",
    "#Find the matches between the 2 images\n",
    "import compareImage as ci\n",
    "\n",
    "matchScore, matches, keypoints1, keypoints2 = ci.find_matches(image1, image2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Sessions\n",
    "You can also compare 2 sessions to each other, depending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session2Directory = \"/Volumes/GeomaticsProjects1/Projects/2025-03 Project FWO SB Jelle/7.Data/21-11 Testbuilding Campus/RAW Data/Hololens/session-2021-11-25 16-17-19\"\n",
    "Session2 = session.Session().from_path(session2Directory)\n",
    "\n",
    "import positioning2D as pos2D\n",
    "\n",
    "bestResults = pos2D.compare_session(singleSession, Session2)\n",
    "\n",
    "print(bestResults.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Transformation matrix\n",
    "The transformation between 2 matched images can be determined using the Essential matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compareImage as ci\n",
    "\n",
    "E, E_direct,F, pts1,pts2, imMatches = ci.calculate_transformation_matrix(image1, image2, matches, keypoints1, keypoints2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangulating the Camera Position\n",
    "When the transformation matrix is calculated, the next step is to determine the camera pose.\n",
    "Because the Essential matrix is correct up to a scale factor, that scale factor needs to be determined first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum distance between known points\n",
    "The first method requires 2 reference images and 1 test image. Since the location of the reference image is known and the direction of the test image is also determined, The location of the test image can be calculated by finding the minimal distance between the 2 estimated positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transform\n",
    "\n",
    "matchScore2, E2, imMatches2 = ci.compare_image(image1, image2)\n",
    "matchScore3, E3, imMatches3 = ci.compare_image(image1, image3)\n",
    "\n",
    "newPos, rot1, pos1, pos2, scale = transform.triangulate_session(image2, image3, E2, E3)\n",
    "\n",
    "print(\"Estimated position:\", newPos)\n",
    "print(\"Actual position:\", image1.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference Session Scaling\n",
    "Since the location of the reference images is known, we can calculate the feature scaling by matching 2 reference images. By then matching the test image to one of the matched images, the scale is already known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchScore, E, imMatches = ci.compare_image(image2, image3)\n",
    "scale = transform.get_session_scale(image2, image3, E)\n",
    "\n",
    "matchScore, E, imMatches = ci.compare_image(image2, image1)\n",
    "direction, rotation = transform.get_translation(E)\n",
    "\n",
    "newPos = image2.pos + direction * scale\n",
    "\n",
    "print(\"The session scale :\", scale)\n",
    "print(\"Estimated position:\", newPos)\n",
    "print(\"Actual position:\", image1.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the 3D scene\n",
    "The session data might also contain 3D data, like a mesh or point cloud. They are also localized, so once the features are calculated for an image, a ray can be cast from a point to determine the global scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning3D\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positioning 3D Functionality\n",
    "Using mesh data to calculate the transformations, the pipeline uses open3D as the main framework for importing, displaying and matching geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing meshes and point clouds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning3D as pos3D\n",
    "import open3d as o3d\n",
    "\n",
    "meshPath = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/ref/mesh-2021-11-25 16-16-01.obj\"\n",
    "mesh = pos3D.import_mesh(meshPath)\n",
    "pos3D.show_geometries([mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning3D as pos3D\n",
    "\n",
    "pcdPath = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/ref/S003-SW-002.ply\"\n",
    "pcd = pos3D.import_point_cloud(pcdPath)\n",
    "\n",
    "pos3D.show_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting and Downsampling\n",
    "Meshes have to be converted to point clouds to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshPath = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/positioning/images/ref/mesh-2021-11-25 16-16-01.obj\"\n",
    "mesh = pos3D.import_mesh(meshPath)\n",
    "nrOfPoints = 10000\n",
    "superSampleFactor = 2\n",
    "pcd = pos3D.to_pcd(mesh, nrOfPoints, superSampleFactor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning Point clouds\n",
    "2 point clouds can be aligned using a FPFH 3D feature matching algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing point cloud from: /Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/ref/pcd1.pcd\n",
      "Importing complete: PointCloud with 100000 points.\n",
      "Importing point cloud from: /Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/ref/S003-SW-002.ply\n",
      "Importing complete: PointCloud with 7638931 points.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Apply fast global registration with distance threshold 0.500\n",
      "This is the estimated transformation: \n",
      " [[-1.95046263e-01 -1.45380299e-02 -9.80686291e-01  5.22049811e+00]\n",
      " [-9.80787476e-01 -7.68041484e-04  1.95077774e-01 -1.10544387e+00]\n",
      " [-3.58925425e-03  9.99894022e-01 -1.41089140e-02  1.96711010e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import positioning3D as pos3D\n",
    "import copy\n",
    "\n",
    "pcd1 = pos3D.import_point_cloud(\"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/ref/pcd1.pcd\")\n",
    "#pcd2 = pos3D.import_point_cloud(\"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/ref/pcd2.pcd\")\n",
    "pcd2 = pos3D.import_point_cloud(\"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/ref/S003-SW-002.ply\")\n",
    "\n",
    "transformation = pos3D.get_pcd_transformation(pcd1, pcd2, 0.1)\n",
    "print(\"This is the estimated transformation: \\n\", transformation)\n",
    "movedPcd = copy.deepcopy(pcd1)\n",
    "movedPcd.transform(transformation)\n",
    "pos3D.show_geometries([pcd1,movedPcd, pcd2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Alignment\n",
    "Once a number of estimated transformations are determined, you can compare them to find the best one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full example\n",
    "This a a workflow example going over all the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import session\n",
    "\n",
    "referencePoint = np.array([0,0,0])\n",
    "maxDistance = 10\n",
    "\n",
    "testSessionDir = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/session-2021-11-25 16-17-19\"\n",
    "refSessionDir = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/session-2021-11-25 16-09-47\"\n",
    "testSession = session.Session().from_path(testSessionDir)\n",
    "referenceSession = session.Session().from_path(refSessionDir)\n",
    "closeEnoughSessions = [referenceSession]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D estimation\n",
    "import positioning2D as pos2D\n",
    "import transform\n",
    "\n",
    "ImageTransformations = pos2D.get_2D_transformation(testSession, closeEnoughSessions)\n",
    "\n",
    "for transformation in ImageTransformations:\n",
    "    print(\"These are the estimated transformations:\")\n",
    "    E2 = transformation[0].transMatrix\n",
    "    E3 = transformation[1].transMatrix\n",
    "    image2 = transformation[0].refImage\n",
    "    image3 = transformation[1].refImage\n",
    "    newPos, rot1, pos1, pos2, scale = transform.triangulate_session(image2, image3, E2, E3)\n",
    "\n",
    "    print(\"Estimated position:\", newPos)\n",
    "    print(\"Actual position:\", transformation[0].testImage.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Comparing: 1 Against 5 Geometries\n",
      "converting mesh to PCD with 100000 points\n",
      "Converting complete PointCloud with 100000 points.\n",
      "converting mesh to PCD with 100000 points\n",
      "Converting complete PointCloud with 100000 points.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Apply fast global registration with distance threshold 0.500\n",
      "converting mesh to PCD with 100000 points\n",
      "Converting complete PointCloud with 100000 points.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Apply fast global registration with distance threshold 0.500\n",
      "converting mesh to PCD with 100000 points\n",
      "Converting complete PointCloud with 100000 points.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Apply fast global registration with distance threshold 0.500\n",
      "converting mesh to PCD with 100000 points\n",
      "Converting complete PointCloud with 100000 points.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Apply fast global registration with distance threshold 0.500\n",
      "converting mesh to PCD with 100000 points\n",
      "Converting complete PointCloud with 100000 points.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Apply fast global registration with distance threshold 0.500\n"
     ]
    }
   ],
   "source": [
    "#3D estimation\n",
    "import positioning3D as pos3D\n",
    "\n",
    "MeshTransformation = pos3D.get_3D_transformation(testSession, closeEnoughSessions, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here are the estimated transformations:\n",
      "[[ 0.69613836  0.38419272 -0.60645473 -7.32581588]\n",
      " [-0.71597569  0.30960796 -0.62571697 -1.09323937]\n",
      " [-0.05263269  0.86979243  0.49060262  5.41529037]\n",
      " [-0.         -0.         -0.          1.        ]]\n",
      "[[  0.73699287  -0.54304891   0.40241693 -11.68871357]\n",
      " [  0.62595815   0.77298948  -0.10326503  -1.96709205]\n",
      " [ -0.25498609   0.32800175   0.90961362   7.88620969]\n",
      " [ -0.          -0.          -0.           1.        ]]\n",
      "[[ 0.69711454  0.01118136  0.71687258 -9.14406207]\n",
      " [ 0.0225307   0.99904289 -0.0374922  -0.14724744]\n",
      " [-0.71660567  0.042288    0.69619541  9.92986617]\n",
      " [-0.         -0.         -0.          1.        ]]\n",
      "[[ -0.36248391  -0.17178131  -0.91602216 -10.31781429]\n",
      " [  0.02834894  -0.98444437   0.17339439  -3.76785748]\n",
      " [ -0.93155877   0.03688442   0.36171507   4.23576794]\n",
      " [  0.           0.          -0.           1.        ]]\n",
      "[[-0.86667598 -0.49313231  0.07545375 -2.00919593]\n",
      " [ 0.49654465 -0.83811859  0.22583321 -5.44931032]\n",
      " [-0.04812646  0.23319037  0.97123946  8.74357556]\n",
      " [-0.         -0.         -0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "for transformations in MeshTransformation:\n",
    "    print(\"here are the estimated transformations:\")\n",
    "    for i, element in enumerate(transformations):\n",
    "        print(element)\n",
    "        transMesh = copy.deepcopy(testSession.geometries[0])\n",
    "        transMesh.transform(element)\n",
    "        pos3D.show_geometries([transMesh, closeEnoughSessions[0].geometries[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transMesh = copy.deepcopy(testSession.geometries[0]).transform(element)\n",
    "pos3D.show_geometries([transMesh, closeEnoughSessions[0].geometries[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing mesh from: /Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/session-2021-11-25 16-17-19/mesh-2021-11-25 16-17-19.obj\n",
      "Importing complete: TriangleMesh with 63197 points and 21839 triangles.\n",
      "converting mesh to PCD with 100000 points\n",
      "Converting complete PointCloud with 100000 points.\n",
      "Importing mesh from: /Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/session-2021-11-25 16-09-47/mesh-2021-11-25 16-16-01.obj\n",
      "Importing complete: TriangleMesh with 525325 points and 183107 triangles.\n",
      "converting mesh to PCD with 100000 points\n",
      "Converting complete PointCloud with 100000 points.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Apply fast global registration with distance threshold 0.500\n",
      "This is the estimated transformation: \n",
      " [[ 9.99998793e-01  1.19977509e-03  9.86784993e-04  4.98610276e-01]\n",
      " [-1.20052025e-03  9.99998994e-01  7.54893234e-04 -2.69928310e-01]\n",
      " [-9.85878298e-04 -7.56076978e-04  9.99999228e-01  8.11919972e+00]\n",
      " [ 0.00000000e+00 -0.00000000e+00 -0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import positioning3D as pos3D\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "meshPath = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/session-2021-11-25 16-17-19/mesh-2021-11-25 16-17-19.obj\"\n",
    "mesh = pos3D.import_mesh(meshPath)\n",
    "nrOfPoints = 100000\n",
    "superSampleFactor = 1\n",
    "pcdTest = pos3D.to_pcd(mesh, nrOfPoints, superSampleFactor)\n",
    "\n",
    "meshPath = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/session-2021-11-25 16-09-47/mesh-2021-11-25 16-16-01.obj\"\n",
    "mesh = pos3D.import_mesh(meshPath)\n",
    "nrOfPoints = 100000\n",
    "superSampleFactor = 1\n",
    "pcdRef = pos3D.to_pcd(mesh, nrOfPoints, superSampleFactor)\n",
    "\n",
    "#pcdRef = pos3D.import_point_cloud(\"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/ref/S003-SW-002.ply\")\n",
    "\n",
    "transformation = pos3D.get_pcd_transformation(pcdTest, pcdRef, 0.1)\n",
    "print(\"This is the estimated transformation: \\n\", transformation)\n",
    "movedPcd = copy.deepcopy(pcdTest)\n",
    "movedPcd.transform(transformation)\n",
    "pos3D.show_geometries([pcdTest,movedPcd, pcdRef],True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "277d597ac5262525bb972cf35caa42eec0dd0b1ee2f46a31453e7fe567547464"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
