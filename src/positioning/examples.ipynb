{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A showcase of the different functionalities of the framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 1: Gather the Test Inputs**\n",
    "  - The test session with all the pictures and coordinates\n",
    "  - The global coordinates of the device\n",
    "- **Step 2: check for relevant Reference data**\n",
    "  - use the global coordinates to find all the reference data that is geo-referenced close enough (GPS precision)\n",
    "- **Step 3: 2D Check**\n",
    "  - Compare all the test images against all the reference images\n",
    "  - Find which session has the highest match rate\n",
    "  - Find which Image has the highest match rate\n",
    "  - Calculate the transformation between the two images\n",
    "  - calculate the inverse transformation to give the test data a Reference global position\n",
    "- **Step 4: 3D Check**\n",
    "  - Compare the test mesh against relevant point clouds\n",
    "  - Compare the test mesh against BIM models\n",
    "  - Perform a CCP for the final alignment\n",
    "- **Step 5: Choosing final Position**\n",
    "  - Use the different results from all the methods to come to a best position\n",
    "  - Send the Position and rotation back to the device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Functionality\n",
    "A session contains all the data from the folder: the images, meshes and their locations. It also contains the geo-reference position and rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Selecting Sessions\n",
    "Sessions can either be directly imported from a path, or be selected from a parent directory based on the distance to the reference point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import session\n",
    "\n",
    "# you can import all the close enough sessions from a parent directory \n",
    "sessionsFolderLocation = \"/Volumes/GeomaticsProjects1/Projects/2025-03 Project FWO SB Jelle/7.Data/21-11 Testbuilding Campus/RAW Data\"\n",
    "referencePoint = np.array([0,0,0])\n",
    "maxDistance = 10\n",
    "sessions = session.find_close_sessions(sessionsFolderLocation, referencePoint, maxDistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import session\n",
    "# you can also import a single session from the directory\n",
    "sessionDirectory = \"/Volumes/GeomaticsProjects1/Projects/2025-03 Project FWO SB Jelle/7.Data/21-11 Testbuilding Campus/RAW Data/Hololens/session-2021-11-25 16-09-47\"\n",
    "singleSession = session.Session().from_path(sessionDirectory)\n",
    "print (singleSession.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Area\n",
    "Each session has a bounding area to determine if the reference position is close enough to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundingBox = singleSession.get_bounding_box()\n",
    "boundingRadius = singleSession.get_bounding_radius()\n",
    "\n",
    "print(\"The axis aligned bounding box (2x3 array) min and max corner points \", boundingBox)\n",
    "print (\"The bounding radius from the reference Center:\", boundingRadius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image transforms\n",
    "The image transform contains the Image file, and it's transform in session space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an image transform is stored in \n",
    "image1 = singleSession.imageTransforms[0]\n",
    "print(image1.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Image\n",
    "The image is stored as an openCV color image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "cv2Image = image1.get_cv2_image()\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.imshow(cv2.cvtColor(cv2Image, cv2.COLOR_BGR2RGB))\n",
    "plt.title('cv2 Color Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Image Camera\n",
    "An image contains it's camera matrix based on the image resolution and vertical fov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 3x3 Camera matrix: \\n\", image1.get_camera_matrix())\n",
    "\n",
    "print(\"The 3x4 camera projection matrix: \\n\", image1.get_projection_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning3D as pos3D\n",
    "import session\n",
    "\n",
    "sessionDirectory = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/session-2021-11-25 16-17-19\"\n",
    "singleSession = session.Session().from_path(sessionDirectory)\n",
    "#print(singleSession.geometries)\n",
    "pos3D.show_geometries(singleSession.geometries, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positioning 2D Functionality\n",
    "All the different methods and functions to calculate the relative position of a Session using 2D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Images\n",
    "2 images can be compared against each other to find the corresponding matches and quality of the match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import session\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "#import 2 images from a session\n",
    "sessionDirectory = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/session-2021-11-25 11-12-17\"\n",
    "singleSession = session.Session().from_path(sessionDirectory)\n",
    "image1 = singleSession.imageTransforms[0]\n",
    "image2 = singleSession.imageTransforms[1]\n",
    "image3 = singleSession.imageTransforms[2]\n",
    "\n",
    "\n",
    "#Find the matches between the 2 images\n",
    "import compareImage as ci\n",
    "\n",
    "match = ci.ImageMatch(image1, image2)\n",
    "match.find_matches()\n",
    "matchImage = match.draw_image_matches()\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.imshow(cv2.cvtColor(matchImage, cv2.COLOR_BGR2RGB))\n",
    "plt.title('cv2 Color Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Sessions\n",
    "You can also compare 2 sessions to each other, depending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session2Directory = \"/Volumes/GeomaticsProjects1/Projects/2025-03 Project FWO SB Jelle/7.Data/21-11 Testbuilding Campus/RAW Data/Hololens/session-2021-11-25 16-17-19\"\n",
    "Session2 = session.Session().from_path(session2Directory)\n",
    "\n",
    "import positioning2D as pos2D\n",
    "\n",
    "bestResults = pos2D.compare_session(singleSession, Session2)\n",
    "\n",
    "print(bestResults.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Transformation matrix\n",
    "The transformation between 2 matched images can be determined using the Essential matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compareImage as ci\n",
    "\n",
    "match = ci.ImageMatch(image1, image2)\n",
    "match.find_matches()\n",
    "match.calculate_transformation_matrix()\n",
    "matchImage = match.draw_image_inliers()\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.imshow(cv2.cvtColor(matchImage, cv2.COLOR_BGR2RGB))\n",
    "plt.title('cv2 Color Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangulating the Camera Position\n",
    "When the transformation matrix is calculated, the next step is to determine the camera pose.\n",
    "Because the Essential matrix is correct up to a scale factor, that scale factor needs to be determined first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum distance between known points\n",
    "The first method requires 2 reference images and 1 test image. Since the location of the reference image is known and the direction of the test image is also determined, The location of the test image can be calculated by finding the minimal distance between the 2 estimated positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transform\n",
    "\n",
    "matchScore2, E2, imMatches2 = ci.compare_image(image1, image2)\n",
    "matchScore3, E3, imMatches3 = ci.compare_image(image1, image3)\n",
    "\n",
    "newPos, rot1, pos1, pos2, scale = transform.triangulate_session(image2, image3, E2, E3)\n",
    "\n",
    "print(\"Estimated position:\", newPos)\n",
    "print(\"Actual position:\", image1.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental Matching\n",
    "Since the location of the reference images is known, we can calculate the feature scaling by matching 2 reference images. By then matching the test image to one of the matched images, the scale is already known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import session\n",
    "\n",
    "#import 2 images from a session\n",
    "sessionDirectory = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/session-2021-11-25 11-12-17\"\n",
    "singleSession = session.Session().from_path(sessionDirectory)\n",
    "image1 = singleSession.imageTransforms[0]\n",
    "image2 = singleSession.imageTransforms[1]\n",
    "image1.fov *=2\n",
    "image2.fov *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: \n",
      " [[ 0.00350176  0.01499996 -0.03647027]\n",
      " [ 0.05957715 -0.03321443 -0.702893  ]\n",
      " [ 0.01997827  0.70595528 -0.03081336]]\n",
      "projectionMatrix: \n",
      " (array([[ 4.15321465e+02,  1.23351091e+01,  1.98901494e+02,\n",
      "         8.61064685e+01],\n",
      "       [ 6.64040508e+01,  3.73440909e+02,  2.24304691e+02,\n",
      "        -1.15680010e+02],\n",
      "       [ 3.15633547e-01,  1.66963748e-02,  9.48734259e-01,\n",
      "        -3.67888242e-01]]), array([[ 3.90517791e+02,  3.60300161e+01,  2.41668309e+02,\n",
      "        -2.37190466e+02],\n",
      "       [ 2.75310134e+01,  3.82985711e+02,  2.16204054e+02,\n",
      "        -9.12523786e+01],\n",
      "       [ 2.09943559e-01,  6.06863553e-02,  9.75828299e-01,\n",
      "        -3.44285152e-01]]))\n"
     ]
    }
   ],
   "source": [
    "#Find the matches between the 2 images\n",
    "import compareImage as ci\n",
    "\n",
    "match = ci.ImageMatch(image1, image2)\n",
    "match.find_matches()\n",
    "match.calculate_transformation_matrix()\n",
    "projectionMatrices = match.get_projectionMatrices(True)\n",
    "#correctKeypoints = match.get_keypoints_from_indices()\n",
    "#print(\"F: \\n\", match.fundamentalMatrix)\n",
    "print(\"E: \\n\", match.essentialMatrix)\n",
    "print(\"projectionMatrix: \\n\", projectionMatrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "matchImage = match.draw_image_inliers()\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.imshow(cv2.cvtColor(matchImage, cv2.COLOR_BGR2RGB))\n",
    "plt.title('cv2 Color Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning3D as pos3D\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import cv2\n",
    "import transform\n",
    "testPoints = match.testInliers.T\n",
    "refPoints = match.refInliers.T\n",
    "#print(testPoints)\n",
    "\n",
    "points3d = cv2.triangulatePoints(projectionMatrices[0], projectionMatrices[1], testPoints, refPoints)\n",
    "points3d = points3d /points3d[3]\n",
    "finalPoints = np.array(points3d[:3]).T\n",
    "ret, rvecs, tvecs, more = cv2.solvePnPRansac(points3d[:3].T, refPoints.T,image2.get_camera_matrix(),np.zeros((1,4)))\n",
    "print(\"rotation: \\n\",cv2.Rodrigues(rvecs)[0],\" \\nTranslation:\\n\", tvecs)\n",
    "ret, rvecs, tvecs, more = cv2.solvePnPRansac(points3d[:3].T, testPoints.T,image1.get_camera_matrix(),np.zeros((1,4)))\n",
    "print(\"rotation: \\n\",cv2.Rodrigues(rvecs)[0],\" \\nTranslation:\\n\", tvecs)\n",
    "\n",
    "#print(ret, cv2.Rodrigues(rvecs), tvecs, more)\n",
    "#cameraPunt = points3d[0].T\n",
    "#print(cameraPunt[0])\n",
    "#print(points3d)\n",
    "\n",
    "#print(finalPoints.T)\n",
    "\n",
    "cam1 = pos3D.create_3d_camera(scale=0.2)\n",
    "R = match.rotationMatrix\n",
    "t = match.translation\n",
    "cvTranslation = np.array(- R.T @ t)\n",
    "cvRotation = R.T\n",
    "print(\"R: \\n\", cvRotation,\"\\n t: \\n\", cvTranslation)\n",
    "cam2 = pos3D.create_3d_camera(cvTranslation,cvRotation , 0.2)\n",
    "cam2_pnp = pos3D.create_3d_camera(tvecs,cv2.Rodrigues(rvecs)[0] , 0.2)\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(finalPoints)\n",
    "#pos3D.show_geometries([image1.get_camera_geometry(), image2.get_camera_geometry(), pcd])\n",
    "pos3D.show_geometries([pcd, cam1, cam2, cam2_pnp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: [[0.35711506]\n",
      " [0.05418119]\n",
      " [0.13471367]]\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import positioning3D as pos3D\n",
    "import numpy as np\n",
    "\n",
    "oldPoints = match.triangulate()\n",
    "match.calculate_scaling_factor()\n",
    "newPoints = match.triangulate(True)\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(oldPoints)\n",
    "pcd2 = o3d.geometry.PointCloud()\n",
    "pcd2.points = o3d.utility.Vector3dVector(newPoints)\n",
    "\n",
    "cam1 = pos3D.create_3d_camera(scale=0.2)\n",
    "cam1_2 = image1.get_camera_geometry(scale=0.2)\n",
    "cam2_2 = image2.get_camera_geometry(scale=0.2)\n",
    "cvTranslation = np.array([image1.pos]).T + np.array( - match.rotationMatrix.T @image1.get_rotation_matrix() @ match.translation)\n",
    "print(\"pos:\", cvTranslation)\n",
    "cvRotation = match.rotationMatrix.T @ image1.get_rotation_matrix()\n",
    "cam2 = pos3D.create_3d_camera(cvTranslation,cvRotation , 0.2)\n",
    "cam2Real = image2.pos - image1.pos, \n",
    "\n",
    "pos3D.show_geometries([pcd,pcd2, cam2_2, cam1_2, cam2], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the 3D scene\n",
    "The session data might also contain 3D data, like a mesh or point cloud. They are also localized, so once the features are calculated for an image, a ray can be cast from a point to determine the global scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning3D\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positioning 3D Functionality\n",
    "Using mesh data to calculate the transformations, the pipeline uses open3D as the main framework for importing, displaying and matching geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing meshes and point clouds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning3D as pos3D\n",
    "import open3d as o3d\n",
    "\n",
    "meshPath = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/ref/mesh-2021-11-25 16-16-01.obj\"\n",
    "mesh = pos3D.import_mesh(meshPath)\n",
    "pos3D.show_geometries([mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning3D as pos3D\n",
    "\n",
    "pcdPath = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/ref/S003-SW-002.ply\"\n",
    "pcd = pos3D.import_point_cloud(pcdPath)\n",
    "\n",
    "pos3D.show_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting and Downsampling\n",
    "Meshes have to be converted to point clouds to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshPath = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/positioning/images/ref/mesh-2021-11-25 16-16-01.obj\"\n",
    "mesh = pos3D.import_mesh(meshPath)\n",
    "nrOfPoints = 10000\n",
    "superSampleFactor = 2\n",
    "pcd = pos3D.to_pcd(mesh, nrOfPoints, superSampleFactor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning Point clouds\n",
    "2 point clouds can be aligned using a FPFH 3D feature matching algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning3D as pos3D\n",
    "import copy\n",
    "\n",
    "pcd1 = pos3D.import_point_cloud(\"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/ref/pcd1.pcd\")\n",
    "#pcd2 = pos3D.import_point_cloud(\"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/ref/pcd2.pcd\")\n",
    "pcd2 = pos3D.import_point_cloud(\"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/ref/S003-SW-002.ply\")\n",
    "\n",
    "transformation = pos3D.get_pcd_transformation(pcd1, pcd2, 0.1)\n",
    "print(\"This is the estimated transformation: \\n\", transformation)\n",
    "movedPcd = copy.deepcopy(pcd1)\n",
    "movedPcd.transform(transformation)\n",
    "pos3D.show_geometries([pcd1,movedPcd, pcd2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Alignment\n",
    "Once a number of estimated transformations are determined, you can compare them to find the best one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full example\n",
    "This a a workflow example going over all the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import session\n",
    "\n",
    "referencePoint = np.array([0,0,0])\n",
    "maxDistance = 10\n",
    "\n",
    "testSessionDir = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/session-2021-11-25 16-17-19\"\n",
    "refSessionDir = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/session-2021-11-25 16-09-47\"\n",
    "testSession = session.Session().from_path(testSessionDir)\n",
    "referenceSession = session.Session().from_path(refSessionDir)\n",
    "closeEnoughSessions = [referenceSession]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D estimation\n",
    "import positioning2D as pos2D\n",
    "import transform\n",
    "\n",
    "ImageTransformations = pos2D.get_2D_transformation(testSession, closeEnoughSessions)\n",
    "\n",
    "for transformation in ImageTransformations:\n",
    "    print(\"These are the estimated transformations:\")\n",
    "    E2 = transformation[0].transMatrix\n",
    "    E3 = transformation[1].transMatrix\n",
    "    image2 = transformation[0].refImage\n",
    "    image3 = transformation[1].refImage\n",
    "    newPos, rot1, pos1, pos2, scale = transform.triangulate_session(image2, image3, E2, E3)\n",
    "\n",
    "    print(\"Estimated position:\", newPos)\n",
    "    print(\"Actual position:\", transformation[0].testImage.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D estimation\n",
    "import positioning3D as pos3D\n",
    "\n",
    "MeshTransformation = pos3D.get_3D_transformation(testSession, closeEnoughSessions, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "for transformations in MeshTransformation:\n",
    "    print(\"here are the estimated transformations:\")\n",
    "    for i, element in enumerate(transformations):\n",
    "        print(element)\n",
    "        transMesh = copy.deepcopy(testSession.geometries[0])\n",
    "        transMesh.transform(element)\n",
    "        pos3D.show_geometries([transMesh, closeEnoughSessions[0].geometries[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transMesh = copy.deepcopy(testSession.geometries[0]).transform(element)\n",
    "pos3D.show_geometries([transMesh, closeEnoughSessions[0].geometries[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning3D as pos3D\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "meshPath = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/session-2021-11-25 16-17-19/mesh-2021-11-25 16-17-19.obj\"\n",
    "mesh = pos3D.import_mesh(meshPath)\n",
    "nrOfPoints = 100000\n",
    "superSampleFactor = 1\n",
    "pcdTest = pos3D.to_pcd(mesh, nrOfPoints, superSampleFactor)\n",
    "\n",
    "meshPath = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/session-2021-11-25 16-09-47/mesh-2021-11-25 16-16-01.obj\"\n",
    "mesh = pos3D.import_mesh(meshPath)\n",
    "nrOfPoints = 100000\n",
    "superSampleFactor = 1\n",
    "pcdRef = pos3D.to_pcd(mesh, nrOfPoints, superSampleFactor)\n",
    "\n",
    "#pcdRef = pos3D.import_point_cloud(\"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/ref/S003-SW-002.ply\")\n",
    "\n",
    "transformation = pos3D.get_pcd_transformation(pcdTest, pcdRef, 0.1)\n",
    "print(\"This is the estimated transformation: \\n\", transformation)\n",
    "movedPcd = copy.deepcopy(pcdTest)\n",
    "movedPcd.transform(transformation)\n",
    "pos3D.show_geometries([pcdTest,movedPcd, pcdRef],True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kerk Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d\n",
    "import cv2\n",
    "import session\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import compareImage as ci\n",
    "from transform import ImageTransform\n",
    "import positioning3D as pos3D\n",
    "import transform\n",
    "import open3d as o3d\n",
    "\n",
    "     \n",
    "\n",
    "cam1_rot = [-0.4816, -0.7658, 0.4259, -0.7635, 0.1282, -0.6329, 0.43, -0.6300,-0.6465]\n",
    "cam1_pos = [3.3354, 8.6431, 2.1599]\n",
    "cam2_rot = [-0.4824, -0.7647, 0.4271, -0.762, 0.1271, -0.6339, 0.4304, -0.6317, -0.6447]\n",
    "cam2_pos = [4.2553, 9.7217, 5.3615]\n",
    "\n",
    "image1 = ImageTransform()\n",
    "image1.path = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/kerk/DJI_0017.JPG\"\n",
    "image1.set_transformation_matrix(cam1_pos, cam1_rot)\n",
    "image1.fov = 105\n",
    "\n",
    "image2 = ImageTransform()\n",
    "image2.path = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/kerk/DJI_0018.JPG\"\n",
    "image2.set_transformation_matrix(cam2_pos, cam2_rot)\n",
    "image2.fov = 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = ci.ImageMatch(image1, image2)\n",
    "match.find_matches()\n",
    "match.calculate_transformation_matrix()\n",
    "projectionMatrices = match.get_projectionMatrices()\n",
    "#correctKeypoints = match.get_keypoints_from_indices()\n",
    "#print(\"F: \\n\", match.fundamentalMatrix)\n",
    "print(\"E: \\n\", match.essentialMatrix)\n",
    "#print(\"projectionMatrix: \\n\", projectionMatrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchImage = match.draw_image_inliers()\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.imshow(cv2.cvtColor(matchImage, cv2.COLOR_BGR2RGB))\n",
    "plt.title('cv2 Color Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPoints = match.testInliers.T\n",
    "refPoints = match.refInliers.T\n",
    "#print(testPoints)\n",
    "\n",
    "points3d = cv2.triangulatePoints(projectionMatrices[0], projectionMatrices[1], testPoints, refPoints)\n",
    "points3d /= points3d[3]\n",
    "finalPoints = np.array(points3d[:3]).T\n",
    "#print(finalPoints)\n",
    "\n",
    "#ret, rvecs, tvecs, more = cv2.solvePnPRansac(points3d[:3].T, refPoints.T,image2.get_camera_matrix(),np.zeros((1,4)))\n",
    "\n",
    "cam1 = pos3D.create_3d_camera(scale=0.2)\n",
    "R = match.rotationMatrix\n",
    "t = match.translation\n",
    "cvTranslation = np.array(- R.T @ t)\n",
    "cvRotation = R.T\n",
    "print(\"R: \\n\", cvRotation,\"\\n t: \\n\", cvTranslation)\n",
    "cam2 = pos3D.create_3d_camera(cvTranslation,cvRotation , 0.2)\n",
    "#cam2 = pos3D.create_3d_camera(np.array([0,-1,0]), R1, 0.2)\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(finalPoints)\n",
    "#pos3D.show_geometries([image1.get_camera_geometry(), image2.get_camera_geometry(), pcd])\n",
    "pos3D.show_geometries([pcd, cam1, cam2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the scene\n",
    "\n",
    "realTranslation = image2.pos - image1.pos\n",
    "realDirection = realTranslation / np.linalg.norm(realTranslation)\n",
    "\n",
    "realLocalTranslation = image1.get_rotation_matrix().T @ realTranslation\n",
    "realLocalDirection = realLocalTranslation / np.linalg.norm(realLocalTranslation)\n",
    "print(\"Rotationmatrix: \\n\", image1.get_rotation_matrix())\n",
    "print(\"local z axis = \\n\", image1.get_rotation_matrix() @ np.array([0,0,1]))\n",
    "print(\"Real translation(lambert): \\n\", realTranslation, \"\\n with direction:\", realDirection)\n",
    "print(\"Real local translation(lambert): \\n\", realLocalTranslation, \"\\n with direction:\", realLocalDirection)\n",
    "print(\"CV translation(Y down): \\n\", cvTranslation.T, \"\\n with direction:\", cvTranslation.T / np.linalg.norm(cvTranslation.T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the 3rd image\n",
    "\n",
    "cam3_rot = [-0.4729, -0.7733, 0.4222, -0.8479, 0.2692, -0.4566, 0.2394, -0.5739, -0.7830]\n",
    "cam3_pos = [4.7772, 8.8708, 13.0912]\n",
    "\n",
    "image3 = ImageTransform()\n",
    "image3.path = \"/Volumes/Data drive/Documents/Doctoraat Local/PythonDataAlignment/src/positioning/images/kerk/DJI_0024.JPG\"\n",
    "image3.set_transformation_matrix(cam3_pos, cam3_rot)\n",
    "image3.fov = 105"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "277d597ac5262525bb972cf35caa42eec0dd0b1ee2f46a31453e7fe567547464"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
