{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating data for Case studies\n",
    "\n",
    "we will generate all the necessary data for the paper in this workbook\n",
    "This means:\n",
    "- 3 case studies, each with their respective test and reference sessions\n",
    "- 2D matching: \n",
    "  - show examples of good and bad matches with the inlier lines and the resulting projection errors\n",
    "  - show the results of each method separately and weighted average over all the test images, this results\n",
    "    - each test returns one estimated pose per test image and or mesh (each time there is a search for the best match)\n",
    "  - compare the estimated global pose against the actual one\n",
    "- 3D matching:\n",
    "  - show example of good and bad matches\n",
    "  - show the correspondences in 3D space with the inlier percent and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#Importing the relevant modules\n",
    "import numpy as np\n",
    "import params\n",
    "\n",
    "import session\n",
    "from imagematch import ImageMatch\n",
    "import positioning3d as pos3d\n",
    "import open3d as o3d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Selection:\n",
    "Pick one of the 3 cases underneath to apply all the functions to the relevant case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 0: Kamer\n",
    "Bedroom for fast testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalPos = np.array([0,0,0])\n",
    "accuraccy = 10\n",
    "\n",
    "testSessionPath = \"/Volumes/Data drive/Documents/Doctoraat Local/XR Paper Data/TestSessions/0_Kamer\"\n",
    "resultPath = \"/Volumes/Data drive/Documents/Doctoraat Local/XR Paper Data/Results/0_Kamer\"\n",
    "# Getting the test session from the device/path\n",
    "testSession = session.Session().from_path(testSessionPath)\n",
    "testSession.convert_axis(\"y\")\n",
    "\n",
    "for image in testSession.imageTransforms:\n",
    "    image.fov *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Campus\n",
    "The technologiecampus Gent, with the focus on the 3 different rooms that were tested: Dubo, Beton, grond\n",
    "The main test here is locating withing a large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalPos = np.array([0,0,0])\n",
    "accuraccy = 10\n",
    "\n",
    "testSessionPath = \"/Volumes/Data drive/Documents/Doctoraat Local/XR Paper Data/TestSessions/1_Campus\"\n",
    "resultPath = \"/Volumes/Data drive/Documents/Doctoraat Local/XR Paper Data/Results/1_Campus\"\n",
    "# Getting the test session from the device/path\n",
    "testSession = session.Session().from_path(testSessionPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: House\n",
    "The house of Maarten Bassier which has had a renovation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalPos = np.array([0,0,0])\n",
    "accuraccy = 10\n",
    "\n",
    "testSessionPath = \"/Volumes/Data drive/Documents/Doctoraat Local/XR Paper Data/TestSessions/2_House\"\n",
    "resultPath = \"/Volumes/Data drive/Documents/Doctoraat Local/XR Paper Data/Results/2_House\"\n",
    "# Getting the test session from the device/path\n",
    "testSession = session.Session().from_path(testSessionPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: LivingLab\n",
    "The living lab house which has been recorded over all the phases of the building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalPos = np.array([0,0,0])\n",
    "accuraccy = 10\n",
    "\n",
    "testSessionPath = \"/Volumes/Data drive/Documents/Doctoraat Local/XR Paper Data/TestSessions/1_Campus\"\n",
    "resultPath = \"/Volumes/Data drive/Documents/Doctoraat Local/XR Paper Data/Results/1_Campus\"\n",
    "# Getting the test session from the device/path\n",
    "testSession = session.Session().from_path(testSessionPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the session objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos3d.show_geometries(testSession.get_session_3d_objects(), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Data Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple reference sessions:\n",
    "With the given global position, only the relevant sessions are stored and parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locating the Reference folder with all the reference sesisons\n",
    "refSessionsPath = \"/Volumes/Data drive/Documents/Doctoraat Local/XR Paper Data/RefSessions\"\n",
    "refSessions = session.find_close_sessions(refSessionsPath, globalPos, accuraccy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Reference Session selection\n",
    "This also has an option to select one single session to compare against\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test with 1 reference session\n",
    "refSessionPath = \"/Volumes/Data drive/Documents/Doctoraat Local/XR Paper Data/RefSessions/KamerJelle/session-2022-02-11 10-02-40\"\n",
    "refSessions = [session.Session().from_path(refSessionPath)]\n",
    "refSessions[0].convert_axis(\"y\")\n",
    "\n",
    "for image in refSessions[0].imageTransforms:\n",
    "    image.fov *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning3d as pos3d\n",
    "pos3d.show_geometries(refSessions[0].get_session_3d_objects(), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Reference Session\n",
    "Remove one image transform from the test session list and use the rest as the reference session against the single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove one image from the list and create a new session\n",
    "nrImage  = 0\n",
    "\n",
    "testImage = testSession.imageTransforms[nrImage]\n",
    "refSession = testSession\n",
    "refSession.imageTransforms = testSession.imageTransforms[:nrImage] + testSession.imageTransforms[nrImage+1:]\n",
    "refSessions = [refSession]\n",
    "testSession = session.Session()\n",
    "testSession.imageTransforms = [testImage]\n",
    "\n",
    "print(refSession.imageTransforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing all the matches\n",
    "\n",
    "this prints out all the match results with all the reference images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagematch import ImageMatch\n",
    "\n",
    "results = []\n",
    "\n",
    "#test every test image against every reference image and get the match object to check the error and confidence\n",
    "for refSession in refSessions:\n",
    "    print(\"Matching reference session:\", refSession.sessionId)\n",
    "\n",
    "    for testImage in testSession.imageTransforms:\n",
    "        print(\"Test Image:\", testImage.id, \"has a matchError of:\")\n",
    "        for refImage in refSession.imageTransforms:\n",
    "            newMatch = ImageMatch(refImage, testImage)\n",
    "            newMatch.find_matches() # find the best matches \n",
    "            results.append(newMatch)\n",
    "            print(newMatch.matchError, \"With image:\", refImage.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good and bad match examples\n",
    "This prints out a very good and very bad match based on the match score, also shows the correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand pick the good and bad matches\n",
    "testImage = testSession.imageTransforms[0]\n",
    "goodImage = testSession.imageTransforms[2]\n",
    "badImage = testSession.imageTransforms[1]\n",
    "\n",
    "bestMatch = ImageMatch(testImage, goodImage)\n",
    "worstMatch = ImageMatch(testImage, badImage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the best and worst match in the list of all the images\n",
    "\n",
    "imageNr = len(testSession.imageTransforms)\n",
    "\n",
    "bestMatch: ImageMatch = None\n",
    "worstMatch: ImageMatch = None\n",
    "\n",
    "for i in range(0,imageNr):\n",
    "    for j in range(i+1, imageNr):\n",
    "        print(i, \" vs. \", j)\n",
    "        newMatch = ImageMatch(testSession.imageTransforms[i], testSession.imageTransforms[j])\n",
    "        newMatch.find_matches() # find the best matches\n",
    "        if(bestMatch == None or bestMatch.matchError > newMatch.matchError):\n",
    "            bestMatch = newMatch\n",
    "        if(worstMatch == None or worstMatch.matchError < newMatch.matchError):\n",
    "            worstMatch = newMatch\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMatch.get_essential_matrix()\n",
    "goodmatches = bestMatch.draw_image_matches()\n",
    "goodinliers = bestMatch.draw_image_inliers()\n",
    "\n",
    "worstMatch.get_essential_matrix()\n",
    "badmatches = worstMatch.draw_image_matches()\n",
    "badinliers = worstMatch.draw_image_inliers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagematch import ImageMatch\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def show_and_save_matches(title, image, error):\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(title + \"\\n Error:\" + str(error))\n",
    "    plt.savefig(os.path.join(resultPath,\"match examples\", title))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "show_and_save_matches('Bad Image Match', badmatches, worstMatch.matchError)\n",
    "show_and_save_matches('Bad Image Inliers', badinliers, worstMatch.matchError)\n",
    "show_and_save_matches('Good Image Match', goodmatches, bestMatch.matchError)\n",
    "show_and_save_matches('Good Image Inliers', goodinliers, bestMatch.matchError)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Match Triangulation\n",
    "with a single match, a 3D scene can be reconstructed, however, the scale of the scene is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "#bestMatch.get_reference_scaling_factor()\n",
    "points = bestMatch.triangulate(True)\n",
    "\n",
    "pcd2 = o3d.geometry.PointCloud()\n",
    "pcd2.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "pos3d.show_geometries([pcd2, bestMatch.image1.get_camera_geometry(), bestMatch.image2.get_camera_geometry(), testSession.geometries[0].get_geometry()], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose estimations method 1 (cross reference matching)\n",
    "Using 2 separate matches to calculate the final pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning2d as pos2d\n",
    "\n",
    "for refSession in refSessions:\n",
    "\n",
    "    for testImage in testSession.imageTransforms:\n",
    "        print(\"Test Image:\", testImage.id)\n",
    "        R,t, matches = pos2d.cross_reference_matching(testImage, refSession)\n",
    "\n",
    "        print(R,t)\n",
    "        testOriginRot = testImage.get_rotation_matrix().T @ R\n",
    "        testOrgininPos = t.T - testImage.pos\n",
    "        testSession.add_pose_guess(refSession, testOriginRot, testOrgininPos, matches, method= \"leastDistance\")\n",
    "        print(\"Estimated pose with confidence:\", 1)\n",
    "        print(\"R: \\n\", testOriginRot, \"\\n t:\\n\", testOrgininPos)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inliers1 =  matches[0].draw_image_inliers()\n",
    "inliers2 =  matches[1].draw_image_inliers()\n",
    "\n",
    "show_and_save_matches(\"match1\", inliers1, matches[0].matchError)\n",
    "show_and_save_matches(\"match1\", inliers2, matches[1].matchError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refcam1 = matches[0].image1.get_camera_geometry(0.2)\n",
    "refcam2 = matches[1].image1.get_camera_geometry(0.2)\n",
    "testCam = matches[0].image2.get_camera_geometry(0.2)\n",
    "\n",
    "estimatedPos = o3d.geometry.TriangleMesh.create_coordinate_frame()\n",
    "estimatedPos.rotate(R)\n",
    "estimatedPos.translate(t)\n",
    "\n",
    "\n",
    "\n",
    "pos3d.show_geometries([refcam1, refcam2, testCam, refSession.geometries[0].get_geometry(), estimatedPos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose estimation method 2 (incremental)\n",
    "using 2 connected matches to find an estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will test every test image against all the reference images and their best match\n",
    "\n",
    "import positioning2d as pos2d\n",
    "\n",
    "for refsession in refSessions:\n",
    "\n",
    "    for testImage in testSession.imageTransforms:\n",
    "        print(\"Test Image:\", testImage.id, \"matched with:\")\n",
    "        R,t, matches = pos2d.incremental_matching(testImage, refSession)\n",
    "\n",
    "        testOriginRot = testImage.get_rotation_matrix().T @ R\n",
    "        testOrgininPos = np.reshape(t.T - testImage.pos, (1,3))\n",
    "        testSession.add_pose_guess(refSession, testOriginRot, testOrgininPos, matches,method=\"incremental\")\n",
    "        #print(\"Estimated pose wit confidence:\", confidence)\n",
    "        print(\"R: \\n\", testOriginRot, \"\\n t:\\n\", testOrgininPos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points = matches[0].points3d\n",
    "points2 = [matches[0].points3d[1]]\n",
    "#pcd = o3d.geometry.PointCloud()\n",
    "#pcd.points = o3d.utility.Vector3dVector(points)\n",
    "pcd2 = o3d.geometry.PointCloud()\n",
    "pcd2.points = o3d.utility.Vector3dVector(points2)\n",
    "colors = []\n",
    "for i, point in enumerate(pcd2.points):\n",
    "    colors.append([0,0,round(1/40*i)])\n",
    "pcd2.colors =o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "print(len(matches[0].points3d))\n",
    "pos3d.show_geometries([pcd2, matches[0].image1.get_camera_geometry(0.2), matches[0].image2.get_camera_geometry(0.2), refSessions[0].geometries[0].get_geometry()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refcam1 = matches[0].image1.get_camera_geometry(0.2)\n",
    "refcam2 = matches[1].image1.get_camera_geometry(0.2)\n",
    "testCam = matches[0].image2.get_camera_geometry(0.2)\n",
    "\n",
    "estimatedPos = o3d.geometry.TriangleMesh.create_coordinate_frame()\n",
    "estimatedPos.rotate(R.T)\n",
    "estimatedPos.translate(- R.T @t)\n",
    "\n",
    "pos3d.show_geometries([refcam1, refcam2, testCam, refSession.geometries[0].get_geometry(), estimatedPos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "\n",
    "trainImage1 =  copy.deepcopy(matches[1].image1.get_cv2_image())\n",
    "QueryImage1 =  copy.deepcopy(matches[0].image1.get_cv2_image())\n",
    "QueryImage0 =  copy.deepcopy(matches[0].image2.get_cv2_image())\n",
    "combinedPoints = matches[0].iterativeMatch\n",
    "print(len(combinedPoints))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def showImage(image,pointList, index):\n",
    "    for i,point in enumerate(pointList):\n",
    "        outImage = cv2.circle(image, np.around(np.asarray(point[index])).astype(int), 30, (0,255,255/len(pointList)*i), -1)\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.imshow(cv2.cvtColor(outImage, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showImage(trainImage1, combinedPoints,0)\n",
    "showImage(QueryImage1, combinedPoints,1)\n",
    "showImage(QueryImage0, combinedPoints,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points = matches[0].points3d\n",
    "points = []\n",
    "for i,point in enumerate(combinedPoints):\n",
    "    points.append(point[3])\n",
    "    #break\n",
    "#pcd = o3d.geometry.PointCloud()\n",
    "#pcd.points = o3d.utility.Vector3dVector(points)\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "pos3d.show_geometries([pcd, matches[1].image1.get_camera_geometry(0.2), matches[1].image2.get_camera_geometry(0.2), refSessions[0].geometries[0].get_geometry()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose estimation method 3 (raycasting)\n",
    "using the 3D scene to get the session scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will test every test image against all the reference images and their best match\n",
    "\n",
    "import positioning2d as pos2d\n",
    "\n",
    "for refsession in refSessions:\n",
    "\n",
    "    for testImage in testSession.imageTransforms:\n",
    "        print(\"Test Image:\", testImage.id, \"matched with:\")\n",
    "        R,t, matches = pos2d.raycast_matching(testImage, refSession) #get the rotation and translation with the pnp point algorithm\n",
    "        print(R,t)\n",
    "        testOriginRot = testImage.get_rotation_matrix().T @ R\n",
    "        testOrgininPos = np.reshape(t.T - testImage.pos, (1,3))\n",
    "        testSession.add_pose_guess(refSession, testOriginRot, testOrgininPos, matches, method= \"raycasting\")\n",
    "        #print(\"Estimated pose wit confidence:\", confidence)\n",
    "        #print(\"R: \\n\", testOriginRot, \"\\n t:\\n\", testOrgininPos.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refcam1 = matches[0].image1.get_camera_geometry(0.2)\n",
    "#refcam2 = matches[1].image1.get_camera_geometry(0.2)\n",
    "testCam = matches[0].image2.get_camera_geometry(0.2)\n",
    "\n",
    "estimatedPos = o3d.geometry.TriangleMesh.create_coordinate_frame()\n",
    "estimatedPos.rotate(R)\n",
    "estimatedPos.translate(t)\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(matches[0].points3d)\n",
    "\n",
    "\n",
    "\n",
    "pos3d.show_geometries([refcam1,testCam,pcd, refSession.geometries[0].get_geometry(), estimatedPos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D alignment\n",
    "\n",
    "This section covers the 3D alignment process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good and bad matches examples\n",
    "\n",
    "shows good and bad matches based on the error and inlier percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying corespondances\n",
    "import open3d as o3d\n",
    "import positioning3d as pos3d\n",
    "from geometrymatch import GeometryMatch\n",
    "from geometrytransform import GeometryTransform\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "pcd1Path = \"/Users/jellevermandere/Downloads/Users/travis/build/nmellado/Super4PCS/Super4PCS-v1.1.3-osx-clang++/assets/roomPart.obj\"\n",
    "pcd2Path = \"/Users/jellevermandere/Downloads/Users/travis/build/nmellado/Super4PCS/Super4PCS-v1.1.3-osx-clang++/assets/roomHolo.obj\"\n",
    "pcd3Path = \"/Volumes/Data drive/Documents/Doctoraat Local/XR Paper Data/RefSessions/KamerJelle/session-2022-02-11 10-02-40/mesh-2022-02-11 10-02-40.obj\"\n",
    "\n",
    "testPcd = GeometryTransform().from_path(pcd1Path)\n",
    "goodPcd = GeometryTransform().from_path(pcd2Path)\n",
    "badPcd = GeometryTransform().from_path(pcd3Path)\n",
    "\n",
    "goodmatch = GeometryMatch(testPcd, goodPcd, 0.05)\n",
    "goodresult = goodmatch.get_transformation()\n",
    "goodlineset = goodmatch.draw_matches()\n",
    "print(\"goodRMSE:\",goodresult.inlier_rmse)\n",
    "print(\"goodFitness:\",goodresult.fitness)\n",
    "badmatch = GeometryMatch(testPcd, badPcd, 0.05)\n",
    "badresult = badmatch.get_transformation()\n",
    "badlineset = badmatch.draw_matches()\n",
    "print(\"badRMSE:\",badresult.inlier_rmse)\n",
    "print(\"badFitness:\",badresult.fitness)\n",
    "\n",
    "#pos3d.show_geometries([testPcd.get_voxel_pcd(),  goodPcd.get_voxel_pcd(),badPcd.get_voxel_pcd(), goodlineset, badlineset], True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodmovedPcd = copy.deepcopy(goodPcd.geometry)\n",
    "goodmovedPcd.transform(goodresult.transformation)\n",
    "goodmovedPcd.compute_vertex_normals()\n",
    "\n",
    "pos3d.show_geometries([testPcd.get_voxel_pcd(), goodmovedPcd,goodPcd.get_voxel_pcd(), goodlineset], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badmovedPcd = copy.deepcopy(badPcd.geometry)\n",
    "badmovedPcd.transform(badresult.transformation)\n",
    "badmovedPcd.compute_vertex_normals()\n",
    "\n",
    "pos3d.show_geometries([testPcd.get_voxel_pcd(), badmovedPcd,badPcd.get_voxel_pcd(), badlineset], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: FPFH feature matching\n",
    "\n",
    "the first method involves matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import session\n",
    "import positioning3d as pos3d\n",
    "\n",
    "# Getting the test session from the device/path\n",
    "#testSessionPath = \"/Volumes/GeomaticsProjects1/Projects/2025-03 Project FWO SB Jelle/7.Data/21-11 Testbuilding Campus/RAW Data/Hololens/session-2021-11-25 16-09-47\"\n",
    "testSessionPath = \"/Volumes/Data drive/Documents/Doctoraat Local/XR Paper Data/TestSessions/0_Kamer\"\n",
    "testSession = session.Session().from_path(testSessionPath)\n",
    "#test with 1 reference session\n",
    "refSessionPath = \"/Volumes/Data drive/Documents/Doctoraat Local/XR Paper Data/RefSessions/KamerJelle/session-2022-02-11 10-02-40\"\n",
    "refSessions = [session.Session().from_path(refSessionPath)]\n",
    "\n",
    "#pos3d.compare_session(testSession, refSession, 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for refSession in refSessions:\n",
    "\n",
    "    for geometry in testSession.geometries:\n",
    "        print(\"Test Image:\", geometry.id)\n",
    "        R,t, matches = pos3d.FPFH_matching(geometry, refSession)\n",
    "        testSession.add_pose_guess(refSession, R, t, matches, method= \"fpfh\")\n",
    "        print(\"Estimated pose:\")\n",
    "        print(\"R: \\n\", R, \"\\n t:\\n\", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the Estimations\n",
    "\n",
    "Display all the estimations, including their confidence in 3d space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning3d as pos3d\n",
    "import open3d as o3d\n",
    "# show all the estimations in 3d space from black to white, by confidence\n",
    "\n",
    "points = []\n",
    "colors = []\n",
    "\n",
    "for estimation in testSession.estimations:\n",
    "    points.append(estimation.position)\n",
    "    colors.append(float(estimation.get_confidence(refSessions[0]))* 4 * np.array([1.0,1.0,1.0]))\n",
    "\n",
    "estimatedPoints = o3d.geometry.PointCloud()\n",
    "estimatedPoints.points = o3d.utility.Vector3dVector(points)\n",
    "estimatedPoints.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "pos3d.show_geometries([estimatedPoints])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import params\n",
    "\n",
    "for i, estimation in enumerate(testSession.estimations):\n",
    "    print(estimation.to_table_string(i, refSessions[0]))\n",
    "    #print (\"Image\", i, estimation.get_confidence(refSessions[0]),\": \\n pos: \\n\", estimation.position.T,\"\\n rot:\\n\", estimation.rotation, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "277d597ac5262525bb972cf35caa42eec0dd0b1ee2f46a31453e7fe567547464"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
