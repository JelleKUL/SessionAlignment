{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A showcase of the different functionalities of the framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 1: Gather the Test Inputs**\n",
    "  - The test session with all the pictures and coordinates\n",
    "  - The global coordinates of the device\n",
    "- **Step 2: check for relevant Reference data**\n",
    "  - use the global coordinates to find all the reference data that is geo-referenced close enough (GPS precision)\n",
    "- **Step 3: 2D Check**\n",
    "  - Compare all the test images against all the reference images\n",
    "  - Find which session has the highest match rate\n",
    "  - Find which Image has the highest match rate\n",
    "  - Calculate the transformation between the two images\n",
    "  - calculate the inverse transformation to give the test data a Reference global position\n",
    "- **Step 4: 3D Check**\n",
    "  - Compare the test mesh against relevant point clouds\n",
    "  - Compare the test mesh against BIM models\n",
    "  - Perform a CCP for the final alignment\n",
    "- **Step 5: Choosing final Position**\n",
    "  - Use the different results from all the methods to come to a best position\n",
    "  - Send the Position and rotation back to the device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Functionality\n",
    "A session contains all the data from the folder: the images, meshes and their locations. It also contains the geo-reference position and rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Selecting Sessions\n",
    "session can either be directly imported fro a path, or be selected from a parent directory based on the distance to the reference point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import session\n",
    "\n",
    "# you can import all the close enough sessions from a parent directory \n",
    "sessionsFolderLocation = \"/Volumes/GeomaticsProjects1/Projects/2025-03 Project FWO SB Jelle/7.Data/21-11 Testbuilding Campus/RAW Data\"\n",
    "referencePoint = np.array([0,0,0])\n",
    "maxDistance = 10\n",
    "sessions = session.find_close_sessions(sessionsFolderLocation, referencePoint, maxDistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also import a single session from the directory\n",
    "sessionDirectory = \"/Volumes/GeomaticsProjects1/Projects/2025-03 Project FWO SB Jelle/7.Data/21-11 Testbuilding Campus/RAW Data/Hololens/session-2021-11-25 16-09-47\"\n",
    "singleSession = session.Session().from_path(sessionDirectory)\n",
    "print (singleSession.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Area\n",
    "Each session has a bounding area to determine if the reference position is close enough to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundingBox = singleSession.get_bounding_box()\n",
    "boundingRadius = singleSession.get_bounding_radius()\n",
    "\n",
    "print(\"The axis aligned bounding box (2x3 array) min and max corner points \", boundingBox)\n",
    "print (\"The bounding radius from the reference Center:\", boundingRadius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image transforms\n",
    "The image transform contains the Image file, and it's transform in session space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an image transform is stored in \n",
    "image1 = singleSession.imageTransforms[0]\n",
    "print(image1.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Image\n",
    "The image is stored as an openCV color image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "cv2Image = image1.get_cv2_image()\n",
    "\n",
    "plt.imshow(cv2.cvtColor(cv2Image, cv2.COLOR_BGR2RGB))\n",
    "plt.title('cv2 Color Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positioning 2D Functionality\n",
    "All the different methods and functions to calculate the relative position of a Session using 2D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Images\n",
    "2 images can be compared against each other to find the corresponding matches and quality of the match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import 2 images from a session\n",
    "sessionDirectory = \"/Volumes/GeomaticsProjects1/Projects/2025-03 Project FWO SB Jelle/7.Data/21-11 Testbuilding Campus/RAW Data/Hololens/session-2021-11-25 16-09-47\"\n",
    "singleSession = session.Session().from_path(sessionDirectory)\n",
    "image1 = singleSession.imageTransforms[0]\n",
    "image2 = singleSession.imageTransforms[1]\n",
    "image3 = singleSession.imageTransforms[2]\n",
    "\n",
    "\n",
    "#Find the matches between the 2 images\n",
    "import compareImage as ci\n",
    "\n",
    "matchScore, matches, keypoints1, keypoints2 = ci.find_matches(image1, image2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Sessions\n",
    "You can also compare 2 sessions to each other, depending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session2Directory = \"/Volumes/GeomaticsProjects1/Projects/2025-03 Project FWO SB Jelle/7.Data/21-11 Testbuilding Campus/RAW Data/Hololens/session-2021-11-25 16-17-19\"\n",
    "Session2 = session.Session().from_path(session2Directory)\n",
    "\n",
    "import positioning2D as pos2D\n",
    "\n",
    "bestResults = pos2D.compare_session(singleSession, Session2)\n",
    "\n",
    "print(bestResults.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Transformation matrix\n",
    "The transformation between 2 matched images can be determined using the Essential matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compareImage as ci\n",
    "\n",
    "E, E_direct,F, pts1,pts2, imMatches = ci.calculate_transformation_matrix(image1, image2, matches, keypoints1, keypoints2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangulating the Camera Position\n",
    "When the transformation matrix is calculated, the next step is to determine the camera pose.\n",
    "Because the Essential matrix is correct up to a scale factor, that scale factor needs to be determined first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum distance between known points\n",
    "The first method requires 2 reference images and 1 test image. Since the location of the reference image is known and the direction of the test image is also determined, The location of the test image can be calculated by finding the minimal distance between the 2 estimated positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transform\n",
    "\n",
    "matchScore2, E2, imMatches2 = ci.compare_image(image1, image2)\n",
    "matchScore3, E3, imMatches3 = ci.compare_image(image1, image3)\n",
    "\n",
    "newPos, rot1, pos1, pos2, scale = transform.triangulate_session(image2, image3, E2, E3)\n",
    "\n",
    "print(\"Estimated position:\", newPos)\n",
    "print(\"Actual position:\", image1.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference Session Scaling\n",
    "Since the location of the reference images is known, we can calculate the feature scaling by matching 2 reference images. By then matching the test image to one of the matched images, the scale is already known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchScore, E, imMatches = ci.compare_image(image2, image3)\n",
    "scale = transform.get_session_scale(image2, image3, E)\n",
    "\n",
    "matchScore, E, imMatches = ci.compare_image(image2, image1)\n",
    "direction, rotation = transform.get_translation(E)\n",
    "\n",
    "newPos = image2.pos + direction * scale\n",
    "\n",
    "print(\"The session scale :\", scale)\n",
    "print(\"Estimated position:\", newPos)\n",
    "print(\"Actual position:\", image1.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the 3D scene\n",
    "The session data might also contain 3D data, like a mesh or point cloud. They are also localized, so once the features are calculated for an image, a ray can be cast from a point to determine the global scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import positioning3D\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positioning 3D Functionality\n",
    "Using mesh data to calculate the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "277d597ac5262525bb972cf35caa42eec0dd0b1ee2f46a31453e7fe567547464"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
